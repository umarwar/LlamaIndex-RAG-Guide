# LlamaIndex: Data Indexing and Retrieval for RAG

This project demonstrates how to use LlamaIndex for building Retrieval-Augmented Generation (RAG) applications with both local and cloud-based vector stores. It includes:

- **Local Quickstart:** Index and chat with your own documents using OpenAI embeddings and LLMs, with persistent local storage.
- **Pinecone Integration:** Store and query document embeddings in Pinecone, leveraging Replicate-hosted LLMs for scalable RAG workflows.
- **Function Agent:** An advanced agent that can answer questions about your documents and perform calculations, using LlamaIndex's function-calling capabilities.

Each notebook and script is self-contained and demonstrates a different aspect of LlamaIndex-powered RAG pipelines.